{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvanzhu/e2e-metrics/blob/master/Kopie_von_nmt_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.statmt.org/europarl/v7/nl-en.tgz\n",
        "!tar -xvzf nl-en.tgz \n",
        "!rm nl-en.tgz"
      ],
      "metadata": {
        "id": "ALeKeY0oEstb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task1&2 Data exploration and preprocessing"
      ],
      "metadata": {
        "id": "ydaq_qIzE5v4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean the empty line with moses scripts"
      ],
      "metadata": {
        "id": "svYvth9LFCeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the lines of unprocessed data\n",
        "#!wc -l europarl-v7.nl-en.en\n",
        "#!wc -l europarl-v7.nl-en.nl\n",
        "''' \n",
        "Strip the empty lines \n",
        "'''\n",
        "!git clone https://github.com/moses-smt/mosesdecoder.git -q\n",
        "! ./mosesdecoder/scripts/training/clean-corpus-n.perl europarl-v7.nl-en en nl europarl-v7.nl-en.no_empty 1 10000"
      ],
      "metadata": {
        "id": "dGvahrhNJwAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight of original empty-line stripped dataset "
      ],
      "metadata": {
        "id": "buAX3sR-FdzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "en = []\n",
        "nl = []\n",
        "with codecs.open('europarl-v7.nl-en.no_empty.en', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "      en.append(line)\n",
        "with codecs.open('europarl-v7.nl-en.no_empty.nl', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "      nl.append(line) "
      ],
      "metadata": {
        "id": "zT2hrO1LFSkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to dataframe\n",
        "import pandas as pd \n",
        "data_ori = pd.DataFrame\n",
        "data_ori = pd.DataFrame (en, columns = ['en'])\n",
        "data_ori['nl'] = pd.DataFrame (nl)\n",
        "data_ori['en'] = data_ori['en'].str.replace('\\n', '')\n",
        "data_ori['nl'] = data_ori['nl'].str.replace('\\n', '')\n",
        "\n",
        "# Get the length of each sentence\n",
        "data_ori[\"en_length\"] = data_ori[\"en\"].apply(lambda x: len(x.split()))\n",
        "data_ori[\"nl_length\"] = data_ori[\"nl\"].apply(lambda x: len(x.split()))\n",
        "\n",
        "# investigate the distribution of the sentence length\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "en_sent = [i for i in data_ori['en_length'] if i != 0]\n",
        "nl_sent = [i for i in data_ori['nl_length'] if i != 0]\n",
        "print('Max length in EN is {}'.format(max(en_sent)))\n",
        "print('Max length in NL is {}'.format(max(nl_sent)))\n",
        "''' As the empty lines are stripped'''\n",
        "plt.hist(en_sent, bins = range(min(en_sent), 200, 1), \n",
        "              alpha = 0.5, color=\"red\")\n",
        "plt.hist(nl_sent, bins = range(min(nl_sent), 200, 1),\n",
        "              alpha = 0.5, color = \"blue\")\n",
        "labels = ['EN',\"NL\"]\n",
        "plt.legend(labels)\n",
        "plt.xlabel(\"length of sentence\")\n",
        "plt.ylabel(\"proportion\")\n",
        "plt.title(\"Sentence length distribution of original dataset\")\n"
      ],
      "metadata": {
        "id": "c3PLoDH1Fbu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean for some place\n",
        "!rm europarl-v7.nl-en.no_empty.*"
      ],
      "metadata": {
        "id": "hgDbcJntWB6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One may know that the long sentence do not improve the translation training. In the following experiments, we decide to compare the performance on length of 15 and 25. We first modify the corpus to file, which only contains sentences with length under 25. Then we will choose approparite ratio to sample the data, in order to make sure it will not out of the capacity of our device."
      ],
      "metadata": {
        "id": "v8WKEntgUmMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "Strip sentences over 25 \n",
        "Set the min length to 3 to avoid some \n",
        "lines only contain the punctuation\n",
        "'''\n",
        "! ./mosesdecoder/scripts/training/clean-corpus-n.perl europarl-v7.nl-en en nl europarl-v7.nl-en.clean15 3 15\n",
        "! ./mosesdecoder/scripts/training/clean-corpus-n.perl europarl-v7.nl-en en nl europarl-v7.nl-en.clean25 3 25"
      ],
      "metadata": {
        "id": "IS-CEn4lFbxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read data"
      ],
      "metadata": {
        "id": "Gjyvhy_MFFWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_15 = []\n",
        "nl_15 = []\n",
        "en_25 = []\n",
        "nl_25 = []\n",
        "with codecs.open('europarl-v7.nl-en.clean15.en', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "      en_15.append(line)\n",
        "with codecs.open('europarl-v7.nl-en.clean15.nl', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "      nl_15.append(line) \n",
        "with codecs.open('europarl-v7.nl-en.clean25.en', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "      en_25.append(line)\n",
        "with codecs.open('europarl-v7.nl-en.clean25.nl', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "      nl_25.append(line) "
      ],
      "metadata": {
        "id": "8T9QHmFUE9zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences length with 25 \n",
        "data25 = pd.DataFrame\n",
        "data25 = pd.DataFrame (en_25, columns = ['en'])\n",
        "data25['nl'] = pd.DataFrame (nl_25)\n",
        "data25['en'] = data25['en'].str.replace('\\n', '')\n",
        "data25['nl'] = data25['nl'].str.replace('\\n', '')\n",
        "data25"
      ],
      "metadata": {
        "id": "c2RqDKouE96Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences length with 25 \n",
        "data15 = pd.DataFrame\n",
        "data15 = pd.DataFrame (en_15, columns = ['en'])\n",
        "data15['nl'] = pd.DataFrame (nl_15)\n",
        "data15['en'] = data15['en'].str.replace('\\n', '')\n",
        "data15['nl'] = data15['nl'].str.replace('\\n', '')\n",
        "data15"
      ],
      "metadata": {
        "id": "TEZtFXoTnhIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 100000 pairs as sampled data\n",
        "sampled_data_25 = data25.sample(n = 100000, ignore_index = True, random_state = 1)\n",
        "sampled_data_15 = data15.sample(n = 100000, ignore_index = True, random_state = 1)"
      ],
      "metadata": {
        "id": "E8XKAAHKYO8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to pair format\n",
        "X = sampled_data_25['en']\n",
        "y = sampled_data_25['nl']\n",
        "\n",
        "with open ('sampled_data_25.ennl', 'w') as f:\n",
        "  for lineen, linenl in zip(X, y):\n",
        "    f.write(lineen+ '\\t'+ linenl+ '\\n')\n",
        "  f.close()\n",
        "\n",
        "X = sampled_data_15['en']\n",
        "y = sampled_data_15['nl']\n",
        "\n",
        "with open ('sampled_data_15.ennl', 'w') as f:\n",
        "  for lineen, linenl in zip(X, y):\n",
        "    f.write(lineen+ '\\t'+ linenl+ '\\n')\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "37U8RiEMcSYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "2Rg7Ee7ckTh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following experiments are not strictly follw the task description. Every detail is explained by text"
      ],
      "metadata": {
        "id": "7maydxJXQvtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_25_path = \"./sampled_data_25.ennl\"\n",
        "data_15_path = \"./sampled_data_15.ennl\""
      ],
      "metadata": {
        "id": "CAMsLSo4dNk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "outputs": [],
      "source": [
        "!pip install sacrebleu -q\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from string import digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrDJ14VplPCP"
      },
      "source": [
        "__Clean and Preprocess the text__\n",
        "1. Convert to lower case\n",
        "2. Remove spaces\n",
        "3. Remove punctuations\n",
        "3. Add start_ and end_ tags to each sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHbppumElPCQ"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def preprocess_sentence(sentence):\n",
        "    #sentence = unicode_to_ascii(sentence.lower().strip())\n",
        "    #num_digits= str.maketrans('','', digits)\n",
        "    sentence= sentence.lower()\n",
        "    sentence= re.sub(\" +\", \" \", sentence)\n",
        "    sentence= re.sub(\"'\", '', sentence)\n",
        "    #sentence= sentence.translate(num_digits)\n",
        "    sentence= sentence.strip()\n",
        "    sentence= re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.rstrip().strip()\n",
        "    sentence= ' '.join(sentence.split())\n",
        "    #for punctuation in string.punctuation:\n",
        "      #sentence = sentence.replace(punctuation, '')\n",
        "    sentence=  'start_ ' + sentence + ' _end'\n",
        "    \n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zcKKAJqlPCR"
      },
      "outputs": [],
      "source": [
        "en_sentence = u\"I will go home at 7 o'clock.\"\n",
        "nl_sentence = u\"Ik ga om 7 uur naar huis.\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(nl_sentence).encode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "outputs": [],
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  \n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  #print(lines)\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  print(path)\n",
        "  return zip(*word_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTbSbBz55QtF"
      },
      "outputs": [],
      "source": [
        "sample_size=80000\n",
        "source, target = create_dataset(data_25_path, sample_size)\n",
        "source_15, target_15 = create_dataset(data_15_path, sample_size)\n",
        "print(source[-1])\n",
        "print(target[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtU7Y4ozlPCU"
      },
      "source": [
        "Create the source and target tokens and post pad them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxnGPvpZlPCV"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 27 # plus start_ and _end\n",
        "source_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "source_sentence_tokenizer.fit_on_texts(source)\n",
        "source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n",
        "source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor,maxlen = MAX_LEN,padding='post' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZMh1L_XlPCV"
      },
      "outputs": [],
      "source": [
        "target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "target_sentence_tokenizer.fit_on_texts(target)\n",
        "target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n",
        "target_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_tensor,maxlen = MAX_LEN,padding='post' )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the max_length of tensor. (MAX_LENGTH = Length +`<START>` + `<END>`)"
      ],
      "metadata": {
        "id": "vZD1r0HPla4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)"
      ],
      "metadata": {
        "id": "ft2Gc1filaIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRvO2p3xlPCW"
      },
      "outputs": [],
      "source": [
        "max_target_length= max(len(t) for t in  target_tensor)\n",
        "print(max_target_length)\n",
        "max_source_length= max(len(t) for t in  source_tensor)\n",
        "print(max_source_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word embedding on source side"
      ],
      "metadata": {
        "id": "AVMBCny81W0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_inp_size = len(source_sentence_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(target_sentence_tokenizer.word_index)+1"
      ],
      "metadata": {
        "id": "yxUv9qff1pTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!rm glove.6B.zip\n",
        "\n",
        "embeddings_file_path = 'glove.6B.300d.txt'"
      ],
      "metadata": {
        "id": "AbjQsQ1V1bpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import array, asarray, zeros\n",
        "\n",
        "embeddings_dict = dict()\n",
        "glove_file = open(embeddings_file_path)\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split() # Turn into array with word on first position and embeddings as rest of line.\n",
        "    word = records[0]\n",
        "    vector_dim = asarray(records[1:], dtype='float32') # Take rest of embeddings out.\n",
        "    embeddings_dict[word] = vector_dim # Add to embeddings_dict as word:embeddings."
      ],
      "metadata": {
        "id": "P0kz-M-A1tP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure your embedding size < word embeddings for each line.\n",
        "EMBEDDING_SIZE = 300\n",
        "word_index_dict = source_sentence_tokenizer.word_index\n",
        "word_count = len(word_index_dict) + 1\n",
        "\n",
        "# num_words is decided either by the minimum word count or the amount of words\n",
        "# in the input sentences, whatever is smaller.\n",
        "#word_count = min(VOCABULARY_SIZE, word_count)\n",
        "\n",
        "# Creates a matrix of zeroes based on the English word count and the \n",
        "# size of the embeddings.\n",
        "embeddings_matrix = zeros((word_count, EMBEDDING_SIZE)) \n",
        "for word, index in word_index_dict.items():\n",
        "    # Attempts to get word embeddings for specific word.\n",
        "    embeddings_vector = embeddings_dict.get(word)\n",
        "    if embeddings_vector is not None:\n",
        "        embeddings_matrix[index] = embeddings_vector\n",
        "word_count"
      ],
      "metadata": {
        "id": "Zfx6aagL1w_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embedding of Target side"
      ],
      "metadata": {
        "id": "AP1OjPi68R-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz\n",
        "!gunzip -d cc.nl.300.vec.gz\n",
        "nl_embeddings_file_path ='cc.nl.300.vec'"
      ],
      "metadata": {
        "id": "6jr3P__8zxRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nl_embeddings_dict = dict()\n",
        "glove_nl_file = open(nl_embeddings_file_path)\n",
        "\n",
        "for line in glove_nl_file:\n",
        "    records = line.split() # Turn into array with word on first position and embeddings as rest of line.\n",
        "    word = records[0]\n",
        "    vector_dim = asarray(records[1:], dtype='float32') # Take rest of embeddings out.\n",
        "    nl_embeddings_dict[word] = vector_dim # Add to embeddings_dict as word:embeddings."
      ],
      "metadata": {
        "id": "ff9pOHqu0mNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure your embedding size < word embeddings for each line.\n",
        "EMBEDDING_SIZE_nl = 300\n",
        "word_index_dict_nl = target_sentence_tokenizer.word_index\n",
        "word_count_nl = len(word_index_dict_nl) + 1\n",
        "\n",
        "# num_words is decided either by the minimum word count or the amount of words\n",
        "# in the input sentences, whatever is smaller.\n",
        "#word_count = min(VOCABULARY_SIZE, word_count)\n",
        "\n",
        "# Creates a matrix of zeroes based on the English word count and the \n",
        "# size of the embeddings.\n",
        "embeddings_matrix_nl = zeros((word_count_nl, EMBEDDING_SIZE_nl)) \n",
        "for word, index in word_index_dict_nl.items():\n",
        "    # Attempts to get word embeddings for specific word.\n",
        "    embeddings_vector = nl_embeddings_dict.get(word)\n",
        "    if embeddings_vector is not None:\n",
        "        embeddings_matrix_nl[index] = embeddings_vector\n",
        "word_count_nl"
      ],
      "metadata": {
        "id": "msDuNbxq09bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zFRUSFUlPCW"
      },
      "source": [
        "##  Creating Train, Validation and Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token-level splitted reference same as tensor split\n",
        "source_train, source_val, target_train, target_val = train_test_split(source, target, test_size=0.4,random_state = 1 )\n",
        "source_val,source_test,target_val,target_test = train_test_split(source_val, target_val, test_size=0.5,random_state = 1 )\n",
        "\n",
        "# Creating training, validation and test sets using an 60-20-20 split\n",
        "source_train_tensor, source_val_tensor, target_train_tensor, target_val_tensor= train_test_split(source_tensor, target_tensor,test_size=0.4,random_state = 1)\n",
        "source_val_tensor, source_test_tensor, target_val_tensor, target_test_tensor = train_test_split(source_val_tensor, target_val_tensor, test_size=0.5,random_state = 1)\n",
        "print(len(source_train_tensor), len(source_val_tensor), len(source_test_tensor))"
      ],
      "metadata": {
        "id": "lC_JEHpNxcDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples of word mapping"
      ],
      "metadata": {
        "id": "4UxBbHYHmCDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        " "
      ],
      "metadata": {
        "id": "ouxOOT-zneYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXukARTDd7MT"
      },
      "outputs": [],
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(source_sentence_tokenizer, source_train_tensor[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert( target_sentence_tokenizer, target_train_tensor[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 27\n",
        "BUFFER_SIZE = len(source_train_tensor)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(source_train_tensor)//BATCH_SIZE\n",
        "steps_per_epoch_val = len(source_val_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(source_sentence_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(target_sentence_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((source_val_tensor, target_val_tensor)).shuffle(BUFFER_SIZE)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        " \n",
        "print('The size of EN vocab is {}'.format(vocab_inp_size))\n",
        "print('The size of NL vocab is {}'.format(vocab_tar_size))\n",
        "print('The steps per training epoch is {}'.format(steps_per_epoch))\n",
        "print('The steps per validation epoch is {}'.format(steps_per_epoch_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc6-NK1GtWQt"
      },
      "outputs": [],
      "source": [
        "# Example batch for configure the model\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## The encoder and decoder model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "EeBwMqf6pUzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, max_len, word_emb = 0,reverse = 0): #word_emb for imply the use of word_embedding\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    if reverse ==1 and word_emb ==1:\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_tar_size,\n",
        "                                               EMBEDDING_SIZE_nl,\n",
        "                                               weights=[embeddings_matrix_nl],\n",
        "                                               input_length=max_len)\n",
        "    if word_emb ==1 and reverse ==0:\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_inp_size,\n",
        "                                               EMBEDDING_SIZE,\n",
        "                                               weights=[embeddings_matrix],\n",
        "                                               input_length=max_len)\n",
        "    if word_emb ==0 and reverse ==0:\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60gSVh05Jl6l"
      },
      "outputs": [],
      "source": [
        "enc = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, MAX_LEN, word_emb = 0, reverse = 0)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = enc.initialize_hidden_state()\n",
        "sample_output, sample_hidden = enc(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.summary()"
      ],
      "metadata": {
        "id": "7okeUKvk7G0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder "
      ],
      "metadata": {
        "id": "lvFtjqB1Emyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, max_len, word_emb = 0, reverse = 0):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    if reverse ==1 and word_emb ==1:\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_inp_size,\n",
        "                                               EMBEDDING_SIZE,\n",
        "                                               weights = [embeddings_matrix],\n",
        "                                               input_length = max_len)\n",
        "    if reverse ==0 and word_emb ==1:\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_tar_size,\n",
        "                                               EMBEDDING_SIZE_nl,\n",
        "                                               weights = [embeddings_matrix_nl],\n",
        "                                               input_length = max_len)\n",
        "    if reverse ==0 and word_emb ==0:\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ],
      "metadata": {
        "id": "lRB7hKICB_h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, MAX_LEN, word_emb = 0,reverse = 0)\n",
        "\n",
        "sample_decoder_output, _= dec(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "metadata": {
        "id": "akw4unt-77Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec.summary()"
      ],
      "metadata": {
        "id": "vToOks-J8Jxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) "
      ],
      "metadata": {
        "id": "SI8-sERBr8Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model = 1024)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "wu1kjsWDsBsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "'''def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)'''\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "outputs": [],
      "source": [
        "#checkpoint_dir = 'training_checkpoints'\n",
        "#checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "#checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "#                                 encoder=encoder,\n",
        "#                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "outputs": [],
      "source": [
        "def train_step(inp, targ, enc_hidden, encoder, decoder, attention = 0, reverse = 0):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    if reverse ==1:\n",
        "      dec_input = tf.expand_dims([source_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)\n",
        "    if reverse ==0:\n",
        "      dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      if attention:\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      else:\n",
        "        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "      \n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        " \n",
        "  return batch_loss "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def caculate_validation_loss(inp, targ, enc_hidden, encoder, decoder, attention = 0, reverse = 0 ):\n",
        "  loss = 0\n",
        "  enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  if reverse ==1:\n",
        "    dec_input = tf.expand_dims([source_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)\n",
        "  if reverse ==0:\n",
        "    dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)\n",
        "\n",
        "  # Teacher forcing - feeding the target as the next input\n",
        "  for t in range(1, targ.shape[1]):\n",
        "    if attention:\n",
        "      predictions, dec_hidden, _  = decoder(dec_input, dec_hidden, enc_output)\n",
        "    else:\n",
        "      predictions, dec_hidden  = decoder(dec_input, dec_hidden, enc_output)\n",
        "    loss += loss_function(targ[:, t], predictions)\n",
        "    dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  loss = loss / int(targ.shape[1])\n",
        " \n",
        "  return loss"
      ],
      "metadata": {
        "id": "xpsQ6J6E1oBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training function\n",
        "def train(epochs, encoder,decoder, attention=0, reverse = 0):\n",
        "  encoder = encoder \n",
        "  decoder = decoder\n",
        "  training_loss  =[]\n",
        "  validation_loss =[]\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    \n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    if reverse ==1:\n",
        "      for (batch, (targ, inp)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss   = train_step(inp, targ, enc_hidden, encoder, decoder, attention, reverse)\n",
        "        total_loss  += batch_loss \n",
        "        if batch % 500 == 0:\n",
        "          print('Epoch {} Batch {} loss {:.4f} '.format(epoch + 1,batch, batch_loss.numpy() ))\n",
        "    if reverse ==0:\n",
        "      for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss   = train_step(inp, targ, enc_hidden, encoder, decoder, attention, reverse)\n",
        "        total_loss  += batch_loss \n",
        "        if batch % 500 == 0:\n",
        "          print('Epoch {} Batch {} loss {:.4f} '.format(epoch + 1,batch, batch_loss.numpy() ))\n",
        "        \n",
        "\n",
        "  \n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_val_loss  = 0\n",
        "    if reverse ==1:\n",
        "      for (batch, (targ, inp)) in enumerate(validation_dataset.take(steps_per_epoch_val)):\n",
        "        val_loss  = caculate_validation_loss(inp, targ, enc_hidden, encoder, decoder, attention, reverse)\n",
        "        total_val_loss  += val_loss \n",
        "    if reverse ==0:\n",
        "      for (batch, (inp, targ)) in enumerate(validation_dataset.take(steps_per_epoch_val)):\n",
        "        val_loss  = caculate_validation_loss(inp, targ, enc_hidden, encoder, decoder, attention, reverse)\n",
        "        total_val_loss  += val_loss \n",
        "\n",
        "    training_loss.append(total_loss / steps_per_epoch)\n",
        "    validation_loss.append(total_val_loss / steps_per_epoch_val)\n",
        "    print('Epoch {} Loss {:.4f} Validation Loss {:.4f}'.format(epoch + 1,\n",
        "                                        training_loss[-1], validation_loss[-1])) \n",
        "\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "  return encoder, decoder, training_loss , validation_loss"
      ],
      "metadata": {
        "id": "ZwFVmA87GHnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('TRAINING WITHOUT ATTENTION LAYER') \n",
        "ENCODER = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, max_len = MAX_LEN, word_emb = 0, reverse = 0)\n",
        "DECODER = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, max_len = MAX_LEN, word_emb = 0, reverse = 0)\n",
        "EPOCHS = 10\n",
        "encoder_no_att, decoder_no_att, train_loss_no_att, val_loss_no_att = train(epochs = EPOCHS, encoder = ENCODER, decoder = DECODER, attention = 0, reverse = 0)\n"
      ],
      "metadata": {
        "id": "kB13AsBPHjDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('TRAINING WITH GLOVE WITHOUT ATTENTION LAYER ') \n",
        "ENCODER = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, max_len = MAX_LEN, word_emb = 1, reverse = 0)\n",
        "DECODER = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, max_len = MAX_LEN, word_emb = 1, reverse = 0)\n",
        "EPOCHS = 10\n",
        "encoder_emb, decoder_emb, train_loss_emb, val_loss_emb = train(epochs = EPOCHS, encoder = ENCODER, decoder = DECODER, attention = 0,reverse = 0)\n"
      ],
      "metadata": {
        "id": "sit5ALG91tPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "ax = plt.subplot(111) \n",
        " \n",
        "t = np.arange(1, epochs+1,1)\n",
        "plt.plot(t, train_loss_no_att ,t, val_loss_no_att , lw=2)\n",
        "plt.plot(t, train_loss_emb,t, val_loss_emb , lw=2, )\n",
        " \n",
        "\n",
        "plt.gcf().set_size_inches(10, 10)\n",
        "ax.set_title(\"Train & Validation loss in 10 epochs\")\n",
        "ax.legend(['Train_loss', 'Val_loss','Train_loss_emb','val_loss_emb'])"
      ],
      "metadata": {
        "id": "eMdiD8rCcqG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tranlation\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token* or when the max traget legth is reached\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ],
      "metadata": {
        "id": "z0JWzt2-asnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence, encoder, decoder, max_len, attention = 0, reverse = 0):\n",
        "  tf.debugging.set_log_device_placement(True)\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    attention_plot = np.zeros((max_len, max_len))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "  #print(sentence)\n",
        "  #print(source_sentence_tokenizer.word_index)\n",
        "    if reverse ==1:\n",
        "      inputs = [target_sentence_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    if reverse ==0:\n",
        "      inputs = [source_sentence_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_len,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    if reverse ==1:\n",
        "      dec_input = tf.expand_dims([source_sentence_tokenizer.word_index['start_']], 0)\n",
        "    if reverse ==0:\n",
        "      dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)\n",
        "\n",
        "    for t in range(max_target_length):\n",
        "      if attention:\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "      else:\n",
        "        predictions, dec_hidden = decoder(dec_input,\n",
        "                                          dec_hidden,\n",
        "                                          enc_out)\n",
        "\n",
        "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "      if reverse ==1:\n",
        "        result += source_sentence_tokenizer.index_word[predicted_id] + ' '\n",
        "      if reverse ==0:\n",
        "        result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
        "      if reverse ==1:\n",
        "        if source_sentence_tokenizer.index_word[predicted_id] == '_end':\n",
        "          if attention: return result, sentence, attention_plot\n",
        "          else: return result, sentence\n",
        "      if reverse ==0:\n",
        "        if target_sentence_tokenizer.index_word[predicted_id] == '_end':\n",
        "          if attention: return result, sentence, attention_plot\n",
        "          else: return result, sentence      \n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    if attention:\n",
        "      return result, sentence, attention_plot\n",
        "    else:\n",
        "      return result, sentence"
      ],
      "metadata": {
        "id": "UzQzxVIYapIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation with only enc-dec\n",
        "result, sentence  = evaluate(u'The Ombudsman must increase its efforts to provide the public with reliable information.', encoder_no_att, decoder_no_att, max_len = MAX_LEN, attention = 0, reverse = 0)\n",
        "result"
      ],
      "metadata": {
        "id": "iV1WK9j6apdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation with enc-dec-emb\n",
        "result, _  = evaluate(u'The Ombudsman must increase its efforts to provide the public with reliable information.', encoder_emb, decoder_emb, max_len = MAX_LEN, attention = 0, reverse = 0)\n",
        "result"
      ],
      "metadata": {
        "id": "_OUnS1PFapiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results with metrics"
      ],
      "metadata": {
        "id": "_O5hmxcIc2bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the translated text of validation set and test set\n",
        "\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  translated_val = []\n",
        "  translated_val_emb = []\n",
        "  for j in source_val[0:1000]:\n",
        "    result, _  = evaluate(j, encoder_no_att, decoder_no_att, max_len = MAX_LEN, attention = 0, reverse = 0)\n",
        "    result_emb, _  = evaluate(j, encoder_emb, decoder_emb, max_len = MAX_LEN, attention = 0, reverse = 0)\n",
        "    translated_val.append(result.replace(' _end',''))\n",
        "    translated_val_emb.append(result_emb.replace(' _end',''))\n",
        "  with open('translation_val.txt','w') as f:\n",
        "    for line in translated_val:\n",
        "      line = line.replace(' _end','')\n",
        "      f.write(line+'\\n')\n",
        "  with open('translation_val_emb.txt','w') as f:\n",
        "    for line in translated_val_emb:\n",
        "      line = line.replace(' _end','')\n",
        "      f.write(line+'\\n')\n",
        "  \n",
        "  translated_test = []\n",
        "  translated_test_emb = []\n",
        "  for i in source_test[0:1000]:\n",
        "    result, _  = evaluate(i, encoder_no_att, decoder_no_att, max_len = MAX_LEN, attention = 0, reverse = 0)\n",
        "    result_emb, _  = evaluate(i, encoder_emb, decoder_emb, max_len = MAX_LEN, attention = 0, reverse = 0)\n",
        "    translated_test.append(result.replace(' _end',''))\n",
        "    translated_test_emb.append(result_emb.replace(' _end',''))\n",
        "  with open('translation_test.txt','w') as f:\n",
        "    for line in translated_test:\n",
        "      line = line.replace(' _end','')\n",
        "      f.write(line+'\\n')\n",
        "  with open('translation_test_emb.txt','w') as f:\n",
        "    for line in translated_test_emb:\n",
        "      line = line.replace(' _end','')\n",
        "      f.write(line+'\\n')\n"
      ],
      "metadata": {
        "id": "XuXnvkLTappE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('reference_val.txt','w') as f:\n",
        "  for line in target_val[0:1000]:\n",
        "    line = line.replace(' _end','')\n",
        "    line = line.replace('start_ ','')\n",
        "    f.write(line+'\\n')\n",
        "\n",
        "with open('reference_test.txt','w') as f:\n",
        "  for line in target_test[0:1000]:\n",
        "    line = line.replace(' _end','')\n",
        "    line = line.replace('start_ ','')\n",
        "    f.write(line+'\\n')"
      ],
      "metadata": {
        "id": "vkgoFdWeaptG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "score :\n",
        "[ bleu\n",
        "chrf2\n",
        "ter ]\n",
        "'''\n",
        "print('Score on Validation set only enc-dec of Bleu, ChrF2 and TER are ')\n",
        "!sacrebleu 'reference_val.txt' -i 'translation_val.txt' -m bleu ter chrf -w 4 -b --force\n",
        "print('Score on Test set only enc-dec of Bleu, ChrF2 and TER are ')\n",
        "!sacrebleu 'reference_test.txt' -i 'translation_test.txt' -m bleu ter chrf -w 4 -b --force\n",
        "print('Score on Validation set only enc-dec+emb of Bleu, ChrF2 and TER are ')\n",
        "!sacrebleu 'reference_val.txt' -i 'translation_val_emb.txt' -m bleu ter chrf -w 4 -b --force\n",
        "print('Score on Test set only enc-dec+emb of Bleu, ChrF2 and TER are ')\n",
        "!sacrebleu 'reference_test.txt' -i 'translation_test_emb.txt' -m bleu ter chrf -w 4 -b --force"
      ],
      "metadata": {
        "id": "9KUe_0ASapy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.2 Investigation on Length\n",
        "So far, the translation based on sentence length of 25 is done. Now we turn to the length of 15 to compare the performance. Just trunctuate the sentence into 15."
      ],
      "metadata": {
        "id": "tHPT0ZQVT497"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN_15 = 17 # plus start_ and _end\n",
        "#source_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "#source_sentence_tokenizer.fit_on_texts(source)\n",
        "source_tensor_15 = source_sentence_tokenizer.texts_to_sequences(source)\n",
        "source_tensor_15= tf.keras.preprocessing.sequence.pad_sequences(source_tensor_15, maxlen = MAX_LEN_15, padding='post' )\n",
        "\n",
        "#target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "#target_sentence_tokenizer.fit_on_texts(target)\n",
        "target_tensor_15 = target_sentence_tokenizer.texts_to_sequences(target)\n",
        "target_tensor_15 = tf.keras.preprocessing.sequence.pad_sequences(target_tensor_15, maxlen = MAX_LEN_15, padding='post' )\n",
        "\n",
        "\n",
        "# Token-level splitted reference same as tensor split\n",
        "source_train_15, source_val_15, target_train_15, target_val_15 = train_test_split(source, target, test_size=0.4,random_state = 1 )\n",
        "source_val_15,source_test_15,target_val_15,target_test_15 = train_test_split(source_val_15, target_val_15, test_size=0.5,random_state = 1 )\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "source_train_tensor_15, source_val_tensor_15, target_train_tensor_15, target_val_tensor_15= train_test_split(source_tensor_15, target_tensor_15,test_size=0.4,random_state = 1)\n",
        "source_val_tensor_15, source_test_tensor_15, target_val_tensor_15, target_test_tensor_15 = train_test_split(source_val_tensor_15, target_val_tensor_15, test_size=0.5,random_state = 1)\n",
        "\n",
        "dataset_15 = tf.data.Dataset.from_tensor_slices((source_train_tensor_15, target_train_tensor_15)).shuffle(BUFFER_SIZE)\n",
        "dataset_15 = dataset_15.batch(BATCH_SIZE, drop_remainder=True)\n",
        "validation_dataset_15 = tf.data.Dataset.from_tensor_slices((source_val_tensor_15, target_val_tensor_15)).shuffle(BUFFER_SIZE)\n",
        "validation_dataset_15 = validation_dataset_15.batch(BATCH_SIZE, drop_remainder=True)\n",
        " \n"
      ],
      "metadata": {
        "id": "QN_w5WEBUMFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training function\n",
        "def train_15(epochs, encoder, decoder, attention=0, reverse = 0):\n",
        "  encoder = encoder \n",
        "  decoder = decoder\n",
        "  training_loss  =[]\n",
        "  validation_loss =[]\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    \n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    for (batch, (inp, targ)) in enumerate(dataset_15.take(steps_per_epoch)):\n",
        "      batch_loss   = train_step(inp, targ, enc_hidden, encoder, decoder, attention, reverse)\n",
        "      total_loss  += batch_loss \n",
        "      if batch % 500 == 0:\n",
        "        print('Epoch {} Batch {} loss {:.4f} '.format(epoch + 1,batch, batch_loss.numpy() ))\n",
        "        \n",
        "\n",
        "  \n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_val_loss  = 0\n",
        "    for (batch, (inp, targ)) in enumerate(validation_dataset_15.take(steps_per_epoch_val)):\n",
        "      val_loss  = caculate_validation_loss(inp, targ, enc_hidden, encoder, decoder, attention, reverse)\n",
        "      total_val_loss  += val_loss \n",
        "\n",
        "    training_loss.append(total_loss / steps_per_epoch)\n",
        "    validation_loss.append(total_val_loss / steps_per_epoch_val)\n",
        "    print('Epoch {} Loss {:.4f} Validation Loss {:.4f}'.format(epoch + 1,\n",
        "                                        training_loss[-1], validation_loss[-1])) \n",
        "\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "  return encoder, decoder, training_loss , validation_loss"
      ],
      "metadata": {
        "id": "mM4BB2iKiNhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('TRAINING WITH GLOVE WITHOUT ATTENTION LAYER L = 15 ') \n",
        "ENCODER = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, MAX_LEN_15, word_emb = 1,reverse = 0)\n",
        "DECODER = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, MAX_LEN_15, word_emb = 1,reverse = 0)\n",
        "EPOCHS = 10\n",
        "encoder_emb_15, decoder_emb_15, train_loss_emb_15, val_loss_emb_15 = train_15(epochs = EPOCHS, encoder = ENCODER, decoder = DECODER, attention = 0)\n"
      ],
      "metadata": {
        "id": "-2qQuPILVApQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "ax = plt.subplot(111) \n",
        " \n",
        "t = np.arange(1, epochs+1,1)\n",
        "plt.plot(t, train_loss_emb_15,t, val_loss_emb_15 , lw=2)\n",
        "plt.plot(t, train_loss_emb,t, val_loss_emb , lw=2, )\n",
        " \n",
        "\n",
        "plt.gcf().set_size_inches(10, 10)\n",
        "ax.set_title(\"Train & Validation loss in 10 epochs\")\n",
        "ax.legend(['Train_loss_L_15', 'Val_loss_L_15','Train_loss_L_25','val_loss_L_25'])"
      ],
      "metadata": {
        "id": "ENrNT72RVAtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the translated text of validation set and test set\n",
        "\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  translated_val_emb = []\n",
        "  sentence_val = []\n",
        "  for j in source_val_15[0:1000]:\n",
        "    result_emb, sentence  = evaluate(j, encoder_emb_15, decoder_emb_15, max_len = MAX_LEN_15, attention = 0, reverse = 0)\n",
        "    translated_val_emb.append(result_emb.replace(' _end',''))\n",
        "    sentence = sentence.replace(' _end','')\n",
        "    sentence_val.append(sentence.replace('start_ ',''))\n",
        "  with open('translation_val_emb_15.txt','w') as f:\n",
        "    for line in translated_val_emb:\n",
        "      line = line.replace(' _end','')\n",
        "      f.write(line+'\\n')\n",
        "  with open('reference_val_15.txt','w') as f:\n",
        "    for line in sentence_val:\n",
        "      f.write(line+'\\n')\n",
        "  \n",
        "  translated_test_emb = []\n",
        "  sentence_test = []\n",
        "  for i in source_test_15[0:1000]:\n",
        "    result_emb, sentence  = evaluate(i, encoder_emb_15, decoder_emb_15, max_len = MAX_LEN_15, attention = 0, reverse = 0)\n",
        "    translated_test_emb.append(result_emb.replace(' _end',''))\n",
        "    sentence = sentence.replace(' _end','')\n",
        "    sentence_test.append(sentence.replace('start_ ',''))\n",
        "  with open('translation_test_emb_15.txt','w') as f:\n",
        "    for line in translated_test_emb:\n",
        "      line = line.replace(' _end','')\n",
        "      f.write(line+'\\n')\n",
        "  with open('reference_test_15.txt','w') as f:\n",
        "    for line in sentence_test:\n",
        "      f.write(line+'\\n')"
      ],
      "metadata": {
        "id": "vm1xkJQQi9Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Score on Validation set only enc-dec+emb_L=15 of Bleu, ChrF2 and TER are ')\n",
        "!sacrebleu 'reference_val_15.txt' -i 'translation_val_emb_15.txt' -m bleu ter chrf -w 4 -b --force\n",
        "print('Score on Test set only enc-dec+emb_L=15 of Bleu, ChrF2 and TER are ')\n",
        "!sacrebleu 'reference_test_15.txt' -i 'translation_test_emb_15.txt' -m bleu ter chrf -w 4 -b --force"
      ],
      "metadata": {
        "id": "1HOMau_Xi9eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 3.3 Reversed Translation"
      ],
      "metadata": {
        "id": "M6o7WJ49No2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('REVERSED TRAINING WITH GLOVE WITHOUT ATTENTION LAYER ') \n",
        "ENCODER = Encoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, MAX_LEN, word_emb = 1,reverse = 1)\n",
        "DECODER = Decoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, MAX_LEN, word_emb = 1,reverse = 1)\n",
        "EPOCHS = 10\n",
        "encoder_reverse, decoder_reverse, train_loss_reverse, val_loss_reverse = train(epochs = EPOCHS, encoder = ENCODER, decoder = DECODER, attention = 0, reverse = 1)\n"
      ],
      "metadata": {
        "id": "Z-6BMnvzNoVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "ax = plt.subplot(111) \n",
        " \n",
        "t = np.arange(1, epochs+1,1)\n",
        "plt.plot(t, train_loss_reverse,t, val_loss_reverse , lw=2)\n",
        "plt.plot(t, train_loss_emb,t, val_loss_emb , lw=2, )\n",
        " \n",
        "\n",
        "plt.gcf().set_size_inches(10, 10)\n",
        "ax.set_title(\"Train & Validation loss in 10 epochs\")\n",
        "ax.legend(['Train_loss_nl-en', 'Val_loss_nl-en','Train_loss_en-nl','val_loss_en-nl'])"
      ],
      "metadata": {
        "id": "kSq-TG1J_xxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, sentence  = evaluate(u'Het is dan ook volkomen juist om door te gaan.',encoder_reverse, decoder_reverse, max_len=MAX_LEN, attention = 0, reverse =1)\n",
        "result\n"
      ],
      "metadata": {
        "id": "Q0hcdZwQnRZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the translated text of validation set and test set\n",
        "\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  translated_val_emb = []\n",
        "  for j in target_val[0:2000]:\n",
        "    result_emb, _  = evaluate(j, encoder_reverse, decoder_reverse, max_len = MAX_LEN, attention = 0, reverse = 1)\n",
        "    translated_val_emb.append(result_emb.replace(' _end',''))\n",
        "  with open('translation_val_reverse.txt','w') as f:\n",
        "    for line in translated_val_emb:\n",
        "      line = line.replace(' _end','')\n",
        "      f.write(line+'\\n')\n",
        "  \n",
        "  translated_test_emb = []\n",
        "  for i in target_test[0:2000]:\n",
        "    result_emb, _  = evaluate(i,encoder_reverse, decoder_reverse, max_len = MAX_LEN, attention = 0, reverse = 1)\n",
        "    translated_test_emb.append(result_emb.replace(' _end',''))\n",
        "\n",
        "  with open('translation_test_reverse.txt','w') as f:\n",
        "    for line in translated_test_emb:\n",
        "      line = line.replace(' _end','')\n",
        "      f.write(line+'\\n')"
      ],
      "metadata": {
        "id": "BHDJ0eLfCC5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('reference_val_reverse.txt','w') as f:\n",
        "  for line in source_val[0:2000]:\n",
        "    line = line.replace(' _end','')\n",
        "    line = line.replace('start_ ','')\n",
        "    f.write(line+'\\n')\n",
        "\n",
        "with open('reference_test_reverse.txt','w') as f:\n",
        "  for line in source_test[0:2000]:\n",
        "    line = line.replace(' _end','')\n",
        "    line = line.replace('start_ ','')\n",
        "    f.write(line+'\\n')"
      ],
      "metadata": {
        "id": "hUK4zj7DCcZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reversed translation from NL to EN')\n",
        "print('Score on Validation set reversed enc-dec+emb_L=25 of Bleu, ChrF2 and TER are ')\n",
        "!sacrebleu 'reference_val_reverse.txt' -i 'translation_val_reverse.txt' -m bleu ter chrf -w 4 -b --force\n",
        "print('Score on Test set reversed enc-dec+emb_L=25 of Bleu, ChrF2 and TER are ')\n",
        "!sacrebleu 'reference_test_reverse.txt' -i 'translation_test_reverse.txt' -m bleu ter chrf -w 4 -b --force"
      ],
      "metadata": {
        "id": "7GuW2A7JCkro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.4 Charater-level tokenizer"
      ],
      "metadata": {
        "id": "HumDTSYiavdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 27 # plus start_ and _end\n",
        "source_char_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='',char_level=True)\n",
        "source_char_tokenizer.fit_on_texts(source)\n",
        "source_char_tensor = source_char_tokenizer.texts_to_sequences(source)\n",
        "source_char_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_char_tensor,padding='post' )"
      ],
      "metadata": {
        "id": "IIf5wbtca3Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 27 # plus start_ and _end\n",
        "target_char_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='',char_level=True)\n",
        "target_char_tokenizer.fit_on_texts(target)\n",
        "target_char_tensor = target_char_tokenizer.texts_to_sequences(target)\n",
        "target_char_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_char_tensor,padding='post' )"
      ],
      "metadata": {
        "id": "Ifgz0Yb_a3Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_target_length_char = max(len(t) for t in  target_char_tensor)\n",
        "print(max_target_length_char)\n",
        "max_source_length_char = max(len(t) for t in  source_char_tensor)\n",
        "print(max_source_length_char)"
      ],
      "metadata": {
        "id": "aKCf86U8a3X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_inp_size_char = len(source_char_tokenizer.word_index)+1\n",
        "vocab_tar_size_char = len(target_char_tokenizer.word_index)+1\n",
        "print(vocab_inp_size_char)\n",
        "print(vocab_tar_size_char)"
      ],
      "metadata": {
        "id": "chlSLns_kbqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('TRAINING WITH GLOVE WITHOUT ATTENTION LAYER ') \n",
        "ENCODER = Encoder(vocab_inp_size_char, embedding_dim, units, BATCH_SIZE, max_len = max_source_length_char, word_emb = 0, reverse = 0)\n",
        "DECODER = Decoder(vocab_tar_size_char, embedding_dim, units, BATCH_SIZE, max_len = max_target_length_char, word_emb = 0, reverse = 0)\n",
        "EPOCHS = 3\n",
        "encoder_char, decoder_char, train_loss_char, val_loss_char = train(epochs = EPOCHS, encoder = ENCODER, decoder = DECODER, attention = 0,reverse = 0)\n"
      ],
      "metadata": {
        "id": "Db1Uisoqkb53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJpT9D5_OgP6"
      },
      "outputs": [],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4 Attention layer"
      ],
      "metadata": {
        "id": "E4YvNBy-bdDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "ZzVhhyaKbbdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "metadata": {
        "id": "CHekYN9Pbbh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionDecoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, max_len, word_emb = 0, reverse = 0):\n",
        "    super(AttentionDecoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    if reverse:\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_inp_size,\n",
        "                                               EMBEDDING_SIZE,\n",
        "                                               weights = [embeddings_matrix],\n",
        "                                               input_length = max_len)\n",
        "    if word_emb:\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_tar_size,\n",
        "                                               EMBEDDING_SIZE_nl,\n",
        "                                               weights = [embeddings_matrix_nl],\n",
        "                                               input_length = max_len)\n",
        "    else:\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x,initial_state = hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "metadata": {
        "id": "tPwcMu-xbbmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_dec = AttentionDecoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, MAX_LEN, word_emb = 0, reverse = 0)\n",
        "\n",
        "sample_decoder_output, _, _ = att_dec(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "metadata": {
        "id": "M3Ndsc5lbkLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_dec.summary()"
      ],
      "metadata": {
        "id": "wE-ywqafbkPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('TRAINING WITH GLOVE WITH ATTENTION LAYER ') \n",
        "ENCODER = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, max_len = MAX_LEN, word_emb = 1, reverse = 0)\n",
        "DECODER = AttentionDecoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, max_len = MAX_LEN, word_emb = 1,reverse = 0)\n",
        "EPOCHS = 5\n",
        "encoder_att, decoder_att, train_loss_att, val_loss_att = train(epochs = EPOCHS, encoder = ENCODER, decoder = DECODER, attention = 1)\n"
      ],
      "metadata": {
        "id": "LeSUHebYbkUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "ax = plt.subplot(111) \n",
        " \n",
        "t = np.arange(1, epochs+1,1)\n",
        "plt.plot(t, train_loss_att,t, val_loss_att, lw=2)\n",
        "plt.plot(t, train_loss_emb,t, val_loss_emb , lw=2, )\n",
        " \n",
        "plt.gcf().set_size_inches(10, 10)\n",
        "ax.set_title(\"Train & Validation loss in 10 epochs\")\n",
        "ax.legend(['Train_loss_att', 'Val_loss_att','Train_loss_no_att','val_loss_no_att'])"
      ],
      "metadata": {
        "id": "xLx6-kxSQJyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  cax = ax.matshow(attention, cmap='bone')\n",
        "  fig.colorbar(cax)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "bbgBrHqvbkZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence,encoder,decoder,attention = 1):\n",
        "  result, sentence, attention_plot = evaluate(sentence,encoder,decoder,attention = 1)\n",
        "  \n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "metadata": {
        "id": "XjELglLJcnUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'The Ombudsman must increase its efforts to provide the public with reliable information.',encoder_att, decoder_att,attention = 1)"
      ],
      "metadata": {
        "id": "rPZDClSKcyv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  translated_val_emb = []\n",
        "  for j in source_val[0:1000]:\n",
        "    result_emb, _ ,_  = evaluate(j, encoder_att, decoder_att, max_len = MAX_LEN, attention = 1, reverse = 0)\n",
        "    translated_val_emb.append(result_emb.replace(' _end',''))\n",
        "  with open('translation_val_att.txt','w') as f:\n",
        "    for line in translated_val_emb:\n",
        "      line = line.replace(' _end','')\n",
        "      f.write(line+'\\n')\n",
        "  \n",
        "  translated_test_emb = []\n",
        "  for i in source_test[0:1000]:\n",
        "    result_emb, _ ,_ = evaluate(i, encoder_att, decoder_att, max_len = MAX_LEN, attention = 1, reverse = 0)\n",
        "    translated_test_emb.append(result_emb.replace(' _end',''))\n",
        "\n",
        "  with open('translation_test_att.txt','w') as f:\n",
        "    for line in translated_test_emb:\n",
        "      line = line.replace(' _end','')\n",
        "      f.write(line+'\\n')\n"
      ],
      "metadata": {
        "id": "QcvDa5lYk7_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Score on Validation set only enc-dec+emb+att of Bleu, ChrF2 and TER are ')\n",
        "!sacrebleu 'reference_val.txt' -i 'translation_val_att.txt' -m bleu ter chrf -w 4 -b --force\n",
        "print('Score on Test set only enc-dec+emb+att of Bleu, ChrF2 and TER are ')\n",
        "!sacrebleu 'reference_test.txt' -i 'translation_test_att.txt' -m bleu ter chrf -w 4 -b --force"
      ],
      "metadata": {
        "id": "NRem4sTNk8KZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Kopie von nmt_with_attention.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}